{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Тестирование кода из коллаба"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"Визуализируем гифочку из набора фрэймов\"\"\"\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    def display_animation(anim):\n",
    "        plt.close(anim._fig)\n",
    "        return HTML(anim.to_jshtml())\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        plt.gcf(), animate, frames = len(frames), interval=50\n",
    "    )\n",
    "\n",
    "    display(display_animation(anim))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\" Простейший буффер реплеев \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
    "        \"\"\"Initializate.\"\"\"\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: np.ndarray,\n",
    "        rew: float,\n",
    "        next_obs: np.ndarray,\n",
    "        done: bool,\n",
    "    ):\n",
    "        \"\"\"Store the transition in buffer.\"\"\"\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "        return dict(obs=self.obs_buf[idxs],\n",
    "                    next_obs=self.next_obs_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\n",
    "\n",
    "    Класс для генерации данных с отклонениями для исследования пространства состояний\n",
    "    Выбранный шум хорошо подходит для физического управления с учетом инерции\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int,\n",
    "        mu: float = 0.0,\n",
    "        theta: float = 0.15,\n",
    "        sigma: float = 0.2,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.state = np.float64(0.0)\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self) -> np.ndarray:\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
    "            [random.random() for _ in range(len(x))]\n",
    "        )\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        init_w: float = 3e-3,\n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(in_dim, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, out_dim)\n",
    "\n",
    "        self.out.weight.data.uniform_(-init_w, init_w)\n",
    "        self.out.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden1(state))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        action = self.out(x).tanh()\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        init_w: float = 3e-3,\n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(in_dim, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 1)\n",
    "\n",
    "        self.out.weight.data.uniform_(-init_w, init_w)\n",
    "        self.out.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(\n",
    "        self, state: torch.Tensor, action: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = torch.cat((state, action), dim=-1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        value = self.out(x)\n",
    "\n",
    "        return value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    \"\"\"DDPGAgent interacting with environment.\n",
    "\n",
    "    Attribute:\n",
    "        env (gym.Env): openAI Gym environment\n",
    "        actor (nn.Module): target actor model to select actions\n",
    "        actor_target (nn.Module): actor model to predict next actions\n",
    "        actor_optimizer (Optimizer): optimizer for training actor\n",
    "        critic (nn.Module): critic model to predict state values\n",
    "        critic_target (nn.Module): target critic model to predict state values\n",
    "        critic_optimizer (Optimizer): optimizer for training critic\n",
    "        memory (ReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        gamma (float): discount factor\n",
    "        tau (float): parameter for soft target update\n",
    "        initial_random_steps (int): initial random action steps\n",
    "        noise (OUNoise): noise generator for exploration\n",
    "        device (torch.device): cpu / gpu\n",
    "        transition (list): temporory storage for the recent transition\n",
    "        total_step (int): total step numbers\n",
    "        is_test (bool): flag to show the current mode (train / test)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        ou_noise_theta: float,\n",
    "        ou_noise_sigma: float,\n",
    "        gamma: float = 0.99,\n",
    "        tau: float = 5e-3,\n",
    "        initial_random_steps: int = 1e4,\n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.env = env\n",
    "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.initial_random_steps = initial_random_steps\n",
    "\n",
    "        # noise\n",
    "        self.noise = OUNoise(\n",
    "            action_dim,\n",
    "            theta=ou_noise_theta,\n",
    "            sigma=ou_noise_sigma,\n",
    "        )\n",
    "\n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.device)\n",
    "\n",
    "        # networks\n",
    "        self.actor = Actor(obs_dim, action_dim).to(self.device)\n",
    "        self.actor_target = Actor(obs_dim, action_dim).to(self.device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        self.critic = Critic(obs_dim + action_dim).to(self.device)\n",
    "        self.critic_target = Critic(obs_dim + action_dim).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # optimizer\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "\n",
    "        # total steps count\n",
    "        self.total_step = 0\n",
    "\n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # if initial random action should be conducted\n",
    "        if self.total_step < self.initial_random_steps and not self.is_test:\n",
    "            selected_action = self.env.action_space.sample()\n",
    "        else:\n",
    "            selected_action = self.actor(\n",
    "                torch.FloatTensor(state).to(self.device)\n",
    "            ).detach().cpu().numpy()\n",
    "\n",
    "        # add noise for exploration during training\n",
    "        if not self.is_test:\n",
    "            noise = self.noise.sample()\n",
    "            selected_action = np.clip(selected_action + noise, -1.0, 1.0)\n",
    "\n",
    "        self.transition = [state, selected_action]\n",
    "\n",
    "        return selected_action\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            self.memory.store(*self.transition)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        device = self.device  # for shortening the following lines\n",
    "\n",
    "        samples = self.memory.sample_batch()\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        action = torch.FloatTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
    "\n",
    "        masks = 1 - done\n",
    "        next_action = self.actor_target(next_state)\n",
    "        next_value = self.critic_target(next_state, next_action)\n",
    "        curr_return = reward + self.gamma * next_value * masks\n",
    "\n",
    "        # train critic\n",
    "        values = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(values, curr_return)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # train actor\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # target update\n",
    "        self._target_soft_update()\n",
    "\n",
    "        return actor_loss.data, critic_loss.data\n",
    "\n",
    "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "\n",
    "        state = self.env.reset()\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        scores = []\n",
    "        score = 0\n",
    "\n",
    "        for self.total_step in tqdm(range(1, num_frames + 1)):\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # if episode ends\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                scores.append(score)\n",
    "                score = 0\n",
    "\n",
    "            # if training is ready\n",
    "            if (\n",
    "                len(self.memory) >= self.batch_size\n",
    "                and self.total_step > self.initial_random_steps\n",
    "            ):\n",
    "                actor_loss, critic_loss = self.update_model()\n",
    "                actor_losses.append(actor_loss.cpu())\n",
    "                critic_losses.append(critic_loss.cpu())\n",
    "\n",
    "            # plotting\n",
    "            if self.total_step % plotting_interval == 0:\n",
    "                self._plot(\n",
    "                    self.total_step,\n",
    "                    scores,\n",
    "                    actor_losses,\n",
    "                    critic_losses,\n",
    "                )\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        frames = []\n",
    "        while not done:\n",
    "            frames.append(self.env.render())\n",
    "            print(state)\n",
    "            action = self.select_action(state[0])\n",
    "            print(action)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def _target_soft_update(self):\n",
    "        \"\"\"Soft-update: target = tau*local + (1-tau)*target.\"\"\"\n",
    "        tau = self.tau\n",
    "\n",
    "        for t_param, l_param in zip(\n",
    "            self.actor_target.parameters(), self.actor.parameters()\n",
    "        ):\n",
    "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
    "\n",
    "        for t_param, l_param in zip(\n",
    "            self.critic_target.parameters(), self.critic.parameters()\n",
    "        ):\n",
    "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
    "\n",
    "    def _plot(\n",
    "        self,\n",
    "        frame_idx: int,\n",
    "        scores: List[float],\n",
    "        actor_losses: List[float],\n",
    "        critic_losses: List[float],\n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "        def subplot(loc: int, title: str, values: List[float]):\n",
    "            plt.subplot(loc)\n",
    "            plt.title(title)\n",
    "            plt.plot(values)\n",
    "\n",
    "        subplot_params = [\n",
    "            (131, f\"frame {frame_idx}. score: {np.mean(scores[-10:])}\", scores),\n",
    "            (132, \"actor_loss\", actor_losses),\n",
    "            (133, \"critic_loss\", critic_losses),\n",
    "        ]\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(30, 5))\n",
    "        for loc, title, values in subplot_params:\n",
    "            subplot(loc, title, values)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class ActionNormalizer(gym.ActionWrapper):\n",
    "    \"\"\"Rescale and relocate the actions.\"\"\"\n",
    "\n",
    "    def action(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Change the range (-1, 1) to (low, high).\"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        scale_factor = (high - low) / 2\n",
    "        reloc_factor = high - scale_factor\n",
    "\n",
    "        action = action * scale_factor + reloc_factor\n",
    "        action = np.clip(action, low, high)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Change the range (low, high) to (-1, 1).\"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        scale_factor = (high - low) / 2\n",
    "        reloc_factor = high - scale_factor\n",
    "\n",
    "        action = (action - reloc_factor) / scale_factor\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "        return action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: 3\n",
      "Action dim: 1\n"
     ]
    }
   ],
   "source": [
    "# environment\n",
    "env_id = \"Pendulum-v1\"\n",
    "env = gym.make(env_id, render_mode = \"rgb_array\")\n",
    "env = ActionNormalizer(env)\n",
    "\n",
    "print(f\"Observation space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Action dim: {env.action_space.shape[0]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "seed_torch(seed)\n",
    "# env.seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "memory_size = 100000\n",
    "batch_size = 128\n",
    "ou_noise_theta = 1.0\n",
    "ou_noise_sigma = 0.1\n",
    "initial_random_steps = 10000\n",
    "\n",
    "agent = DDPGAgent(\n",
    "    env,\n",
    "    memory_size,\n",
    "    batch_size,\n",
    "    ou_noise_theta,\n",
    "    ou_noise_sigma,\n",
    "    initial_random_steps=initial_random_steps\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.74943817,  0.6620743 , -0.5586284 ], dtype=float32), {})\n",
      "[0.0013633]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [56], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# test without training\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m frames \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mtest()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# display\u001B[39;00m\n\u001B[0;32m      4\u001B[0m display_frames_as_gif(frames)\n",
      "Cell \u001B[1;32mIn [51], line 202\u001B[0m, in \u001B[0;36mDDPGAgent.test\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    200\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselect_action(state[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28mprint\u001B[39m(action)\n\u001B[1;32m--> 202\u001B[0m next_state, reward, done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    204\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[0;32m    205\u001B[0m score \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n",
      "Cell \u001B[1;32mIn [51], line 101\u001B[0m, in \u001B[0;36mDDPGAgent.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mfloat64, \u001B[38;5;28mbool\u001B[39m]:\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;124;03m\"\"\"Take an action and return the response of the env.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 101\u001B[0m     next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_test:\n\u001B[0;32m    104\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransition \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [reward, next_state, done]\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# test without training\n",
    "frames = agent.test()\n",
    "# display\n",
    "display_frames_as_gif(frames)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "[1]"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
