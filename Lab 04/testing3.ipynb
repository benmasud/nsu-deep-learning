{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "log_path = os.path.join('Training', 'Logs')\n",
    "model_path = os.path.join('Training', 'Saved Models')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "env = make_atari_env(\"ALE/Breakout-v5\", n_envs=4, seed=SEED)\n",
    "env = VecFrameStack(env, n_stack=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# episodes = 5\n",
    "# for episode in range(episodes):\n",
    "#     state = env.reset()\n",
    "#     done = np.array([False])\n",
    "#\n",
    "#     while not done.all():\n",
    "#         env.render()\n",
    "#         action = env.action_space.sample()\n",
    "#         n_state, reward, done, info = env.step([action]*os.cpu_count())\n",
    "#         print(done)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(model_path, 'PPO_100k.zip')):\n",
    "    model = PPO.load(os.path.join(model_path, 'PPO_100k.zip'), env=env)\n",
    "else:\n",
    "    model.learn(100000)\n",
    "    model.save(os.path.join(model_path, 'PPO_100k'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(20.4, 3.7735924528226414)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model=model, env=env, render=True, n_eval_episodes=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(model_path, 'PPO_1M.zip')):\n",
    "    model = PPO.load(os.path.join(model_path, 'PPO_1M.zip'), env=env)\n",
    "else:\n",
    "    model.learn(1000000)\n",
    "    model.save(os.path.join(model_path, 'PPO_1M'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(14.5, 6.606814663663572)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model=model, env=env, render=True, n_eval_episodes=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "стоило сразу коллбэк повесить, а то произошли сильные просадки про скору и надо вытащить из этой ямы"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "env.reset()\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=20, verbose=1)\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=stop_callback,\n",
    "    eval_freq=10000,\n",
    "    best_model_save_path=model_path,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:403: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000001A759F36A10> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x000001A5B2F1AFB0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 700      |\n",
      "|    ep_rew_mean     | 17       |\n",
      "| time/              |          |\n",
      "|    fps             | 219      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 665       |\n",
      "|    ep_rew_mean          | 15.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 90        |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4739682 |\n",
      "|    clip_fraction        | 0.439     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.457    |\n",
      "|    explained_variance   | 0.579     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0371   |\n",
      "|    n_updates            | 1370      |\n",
      "|    policy_gradient_loss | -0.0674   |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 661       |\n",
      "|    ep_rew_mean          | 15.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 173       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 141       |\n",
      "|    total_timesteps      | 24576     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4109888 |\n",
      "|    clip_fraction        | 0.439     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.501    |\n",
      "|    explained_variance   | 0.659     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.000885 |\n",
      "|    n_updates            | 1380      |\n",
      "|    policy_gradient_loss | -0.0657   |\n",
      "|    value_loss           | 0.192     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 669        |\n",
      "|    ep_rew_mean          | 16.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 193        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45296538 |\n",
      "|    clip_fraction        | 0.443      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.487     |\n",
      "|    explained_variance   | 0.685      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.064     |\n",
      "|    n_updates            | 1390       |\n",
      "|    policy_gradient_loss | -0.0656    |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=15.40 +/- 4.59\n",
      "Episode length: 681.60 +/- 128.11\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 682        |\n",
      "|    mean_reward          | 15.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43245804 |\n",
      "|    clip_fraction        | 0.444      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.497     |\n",
      "|    explained_variance   | 0.706      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0936    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0709    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 675      |\n",
      "|    ep_rew_mean     | 16.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 249      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 653        |\n",
      "|    ep_rew_mean          | 15.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 301        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42279252 |\n",
      "|    clip_fraction        | 0.448      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.5       |\n",
      "|    explained_variance   | 0.738      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 1410       |\n",
      "|    policy_gradient_loss | -0.0694    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 654       |\n",
      "|    ep_rew_mean          | 15.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 162       |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 353       |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4689479 |\n",
      "|    clip_fraction        | 0.444     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.477    |\n",
      "|    explained_variance   | 0.72      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0168   |\n",
      "|    n_updates            | 1420      |\n",
      "|    policy_gradient_loss | -0.069    |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 674       |\n",
      "|    ep_rew_mean          | 15.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 161       |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 406       |\n",
      "|    total_timesteps      | 65536     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5126863 |\n",
      "|    clip_fraction        | 0.437     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.462    |\n",
      "|    explained_variance   | 0.705     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0116   |\n",
      "|    n_updates            | 1430      |\n",
      "|    policy_gradient_loss | -0.0687   |\n",
      "|    value_loss           | 0.153     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 680       |\n",
      "|    ep_rew_mean          | 16.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 161       |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 457       |\n",
      "|    total_timesteps      | 73728     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5355576 |\n",
      "|    clip_fraction        | 0.451     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.472    |\n",
      "|    explained_variance   | 0.717     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0519   |\n",
      "|    n_updates            | 1440      |\n",
      "|    policy_gradient_loss | -0.0669   |\n",
      "|    value_loss           | 0.177     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=13.20 +/- 4.31\n",
      "Episode length: 613.20 +/- 141.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 613        |\n",
      "|    mean_reward          | 13.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 80000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46834362 |\n",
      "|    clip_fraction        | 0.43       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.46      |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0676    |\n",
      "|    n_updates            | 1450       |\n",
      "|    policy_gradient_loss | -0.0652    |\n",
      "|    value_loss           | 0.15       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 666      |\n",
      "|    ep_rew_mean     | 16.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 512      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 647        |\n",
      "|    ep_rew_mean          | 15.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 565        |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42983985 |\n",
      "|    clip_fraction        | 0.42       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.479     |\n",
      "|    explained_variance   | 0.743      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0373    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0612    |\n",
      "|    value_loss           | 0.199      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 657        |\n",
      "|    ep_rew_mean          | 15.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 618        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44603828 |\n",
      "|    clip_fraction        | 0.449      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.481     |\n",
      "|    explained_variance   | 0.715      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0941    |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | -0.0667    |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 700        |\n",
      "|    ep_rew_mean          | 17.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 669        |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49917412 |\n",
      "|    clip_fraction        | 0.439      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.462     |\n",
      "|    explained_variance   | 0.677      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0858    |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0644    |\n",
      "|    value_loss           | 0.178      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.ppo.ppo.PPO at 0x1a588056ad0>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(100000, callback=eval_callback)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(17.92, 6.045957327007858)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "best = PPO.load(os.path.join(model_path, 'best_model.zip'), env=env)\n",
    "evaluate_policy(model=best, env=env, render=True, n_eval_episodes=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Во время тренировки были пики со скорами ~20, можно было там сохранить. Так же можно поподбирать гиперпараметры, но допустим что это хоть какой-то результат для первой сдачи, поэтому пока оставлю так."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
